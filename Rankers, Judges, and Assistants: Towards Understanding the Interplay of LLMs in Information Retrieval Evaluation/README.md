## RANKERS, JUDGES, AND ASSISTANTS: TOWARDS UNDERSTANDING THE INTERPLAY OF LLMS IN INFORMATION RETRIEVAL EVALUATION
https://dl.acm.org/doi/10.1145/3726302.3730348

### **1. What is the problem we're looking at?**

Large Language Models (LLMs), like the AI models powering modern chatbots and search engines, are increasingly used in two key roles within information retrieval (IR) systems:

- As **rankers**: deciding which search results are most relevant to a user's query.
- As **judges**: automatically evaluating or scoring how relevant those results are—often standing in for human assessors.

But there's a catch: If both the ranking and the judging are done by LLMs, are we evaluating systems fairly? Or could there be hidden biases and issues? Until now, research only hinted at these problems; we want to dig deeper, especially when LLMs are used both to produce and to evaluate ranking results.

### **2. Why is this important?**

If LLM judges favor systems that use similar LLM technology—maybe because of shared reasoning or stylistic similarities—we might end up with evaluations that are unfairly "gaming" the system. This could hide weaknesses in the models, suppress diversity in search results, or unfairly penalize new ideas that don't fit the LLM's training.

### **3. What did we do?**

We:

- Reviewed how LLMs are currently used in IR, emphasizing that these roles are deeply interconnected.
- Ran experiments comparing human judges vs. various LLM judges, using recent TREC Deep Learning datasets of search queries, with human-graded relevance as the "ground truth."
- Compared systems using both LLM-based rankers and traditional (non-LLM) methods.
- Explored not just LLM-judged ranking, but also what happens when the documents being ranked are themselves generated or rewritten by LLMs.

### **4. What did we find?**

**a) LLM judges are more lenient than humans.**  
When giving relevance scores, LLM judges tend to be more forgiving or "washed out" than strict human judges.

**b) LLM judges are biased toward LLM-based rankers.**  
When ranking systems are LLM-based, LLM judges prefer their results—even if, according to human judges and ground truth, these aren't the best results. Sometimes, this bias is so strong that the real "gold standard" results (according to humans) are ranked below all LLM-based systems by an LLM judge.

**c) LLMs struggle to spot subtle but significant differences.**  
When the difference in ranking quality between two systems is small—but statistically meaningful (according to humans)—LLM judges often fail to pick this up.

**d) Bias toward LLM-generated content is harder to prove.**  
We checked whether LLM judges prefer texts that have been rewritten or generated by other LLMs (even when the ranking method is not LLM-based). In our specific experiments, we found little evidence for this kind of bias, though prior work claims it exists. So, the jury is still out, and more research is needed.

### **5. Why are these findings important?**

If LLM judges are replacing humans, but are systematically biased or "favor their own kind," then the entire evaluation process risks becoming a feedback loop—rewarding systems that produce outputs similar to what LLMs expect (or are trained on), even if those are not what real users want.

Worse, if we use LLM-generated labels to train new LLM-based systems ("circularity"), it can make problems snowball: systems learn to optimize for LLM judges rather than real-world usefulness.

### **6. What do we recommend?**

- **LLM judges should not simply replace humans.** LLMs can help speed up evaluation and reduce human workload, but human judgment remains the true gold standard.
- **Consistency is key.** Always use the same LLM judge and settings across comparisons.
- **Be transparent.** Clearly report which LLM model, version, prompt, and config you use.
- **Use LLM judges as filters, not final arbiters.** LLMs can help narrow down which systems are worth deeper, human review, but shouldn't be the only metric.

### **7. What are the open questions or next steps?**

- How do these biases play out in the wild, especially with different LLM models and more varied evaluation setups?
- How can we better detect and quantify these biases so that system evaluation remains fair and useful?

### **In summary:**  
Using LLMs as both rankers and judges in IR creates risks of hidden biases and feedback loops that can mislead us about true system quality. We provide empirical evidence of these biases, offer early guidelines to help the community, and call for more research to ensure future IR evaluation methods serve real users—not just the algorithms judging themselves