# WildChat: A Benchmark of Unfiltered, Real-World User Interactions

**URL:** [https://sigir2025.dei.unipd.it/detailed-program/paper?paper=49182f81e6a13cf5eaa496d51fea6406](https://sigir2025.dei.unipd.it/detailed-program/paper?paper=49182f81e6a13cf5eaa496d51fea6406)

## The "Testing in the Real World" Paper

### The Feynman Explanation:

A student can get a perfect score on a textbook Spanish test but be totally unable to order a coffee in Madrid. The test in the classroom is clean and simple. The real world is messy, fast, and unpredictable.

Many LLM benchmarks are like classroom tests. They are clean, simple questions. But in the real world, people type with typos, change their minds mid-sentence, and ask weird, complex questions.

**This paper's idea is to collect a massive dataset of how real people _actually_ talk to chatbots in the wild.** They take thousands of anonymous, messy, real-world conversations and use them as a new test. Can the LLM handle a user who says "uhhh nvm can u tell me about... not that, the other thing"?

**The Punchline:** This measures how a model performs in realistic situations, closing the gap between lab scores and real-world utility. It provides a more **consistent predictor of how helpful a model will actually be** for an average person.

## Problem Addressed & Summary

Bridges the gap between clean, artificial benchmarks and messy real-world usage. Introduces a dataset of authentic, unfiltered human-chatbot conversations, capturing typos, changes of intent, and ambiguity. Provides a robust test of practical usefulness and user satisfaction outside controlled lab settings.