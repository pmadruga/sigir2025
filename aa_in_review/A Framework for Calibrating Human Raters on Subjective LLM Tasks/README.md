# A Framework for Calibrating Human Raters on Subjective LLM Tasks

**URL:** [https://sigir2025.dei.unipd.it/detailed-program/paper?paper=61b4a64be663682e8cb037d9719ad8cd](https://sigir2025.dei.unipd.it/detailed-program/paper?paper=61b4a64be663682e8cb037d9719ad8cd)

## The "Making Human Judges Fair" Paper

### The Feynman Explanation:

Let's say you're judging an essay contest. You have three judges.

- Judge 1 is a tough grader. The best essay gets a B.
- Judge 2 is an easy grader. Every decent essay gets an A.
- Judge 3 loves creative writing but hates formal arguments.

Their scores will be all over the place! You can't reliably say which essay is best. To fix this, you'd give them a **rubric**—a detailed set of rules for how to score things like "clarity," "creativity," and "correctness." You'd have them all grade the same three practice essays first and discuss their scores until they all agree. This is called **calibration**.

**This paper's idea is to create a strict rulebook and training process for humans who evaluate LLMs.** It makes sure that when a human rates an LLM's response as "good," it means the same thing, no matter who the human is.

**The Punchline:** This makes human feedback, which is crucial for judging things like creativity and helpfulness, far more **consistent and reliable**. You can actually trust the scores.

## Problem Addressed & Summary

Focuses on reducing inconsistency and subjectivity in human evaluation of LLM outputs. Defines rigorous rubrics and calibration processes so "good" means the same to every judge. Delivers fairer, more reliable human scores—critical for evaluating nuanced concepts like creativity or helpfulness.