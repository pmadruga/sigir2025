## Overview

| Paper & Link | Problem Addressed & Summary |
|---|---|
| **HELM-D: A Dynamic Benchmark for Continual LLM Evaluation**<br>[Link](https://sigir2025.dei.unipd.it/detailed-program/paper?paper=6855456e2fe46a9d49d3d3af4f57443d) | Tackles "teaching to the test" in LLM evaluation—where models are overfitting to static, public benchmarks.<br>Introduces a continually updated, dynamic benchmark that prevents memorization and encourages true generalization.<br>Ensures model scores stay meaningful and trustworthy as benchmarks evolve and models improve over time. |
| **A Framework for Calibrating Human Raters on Subjective LLM Tasks**<br>[Link](https://sigir2025.dei.unipd.it/detailed-program/paper?paper=61b4a64be663682e8cb037d9719ad8cd) | Focuses on reducing inconsistency and subjectivity in human evaluation of LLM outputs.<br>Defines rigorous rubrics and calibration processes so "good" means the same to every judge.<br>Delivers fairer, more reliable human scores—critical for evaluating nuanced concepts like creativity or helpfulness. |
| **Red-Teaming at Scale: Systematically Discovering LLM Failure Modes**<br>[Link](https://sigir2025.dei.unipd.it/detailed-program/paper?paper=fc49306d97602c8ed1be1dfbf0835ead) | Applies systematic "red-teaming"—crash testing—to reveal LLM weaknesses in safety and reliability.<br>Develops large, organized banks of adversarial prompts targeting bias, misinformation, and harmful outputs.<br>Measures model consistency from a safety angle, spotlighting issues that only appear under stress or attempted misuse. |
| **Evaluating Reasoning Pathways, Not Just Final Answers**<br>[Link](https://sigir2025.dei.unipd.it/detailed-program/paper?paper=5680522b8e2bb01943234bce7bf84534) | Argues for grading a model's reasoning process ("chain of thought"), not just its final answer.<br>Evaluates if the logical steps make sense, distinguishing genuine reasoning from lucky guesses or memorization.<br>Yields a truer, more consistent measure of LLM intelligence and reasoning ability. |
| **Assessing the Viability and Bias of LLM-based Evaluators**<br>[Link](https://sigir2025.dei.unipd.it/detailed-program/paper?paper=01d8bae291b1e4724443375634ccfa0e) | Explores using powerful LLMs as automated judges to grade other models' answers, speeding up evaluation.<br>Compares LLM-judge scores to calibrated human experts to check consistency and detect biases (e.g., favoritism).<br>If proven reliable, enables rapid, massive-scale LLM benchmarking with confidence in fairness. |
| **WildChat: A Benchmark of Unfiltered, Real-World User Interactions**<br>[Link](https://sigir2025.dei.unipd.it/detailed-program/paper?paper=49182f81e6a13cf5eaa496d51fea6406) | Bridges the gap between clean, artificial benchmarks and messy real-world usage.<br>Introduces a dataset of authentic, unfiltered human-chatbot conversations, capturing typos, changes of intent, and ambiguity.<br>Provides a robust test of practical usefulness and user satisfaction outside controlled lab settings. |